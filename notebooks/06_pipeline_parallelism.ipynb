{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Parallelism\n",
    "ì´ë²ˆ ì„¸ì…˜ì—ì„œëŠ” íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## 1. Inter-layer model parallelism\n",
    "íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”ëŠ” Inter-layer ëª¨ë¸ ë³‘ë ¬í™”ë¥¼ ê°œì„ í•œ ê²ƒì…ë‹ˆë‹¤. Inter-layer ëª¨ë¸ ë³‘ë ¬í™”ëŠ” ì•„ë˜ì™€ ê°™ì´ íŠ¹ì • GPUì— íŠ¹ì • ë ˆì´ì–´ë“¤ì„ í• ë‹¹í•˜ëŠ” ëª¨ë¸ ë³‘ë ¬í™” ë°©ë²•ì´ì˜€ì£ . ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” GPU1ë²ˆì— 1,2,3ë²ˆ ë ˆì´ì–´ê°€ í• ë‹¹ë˜ì—ˆê³ , GPU2ë²ˆì— 4,5ë²ˆ ë ˆì´ì–´ê°€ í• ë‹¹ ë˜ì—ˆëŠ”ë°, ì´ ë•Œ ìª¼ê°œì§„ í•˜ë‚˜ì˜ ì¡°ê°ì„ `stage(ìŠ¤í…Œì´ì§€)`ë¼ê³  í•©ë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œì˜ ê²½ìš° 2ê°œì˜ ìŠ¤í…Œì´ì§€ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](../images/inter_layer.png)\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜ ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ì„ ë‹¤ìŒ ë ˆì´ì–´ì˜ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ì‹ ê²½ë§ì˜ íŠ¹ì„±ìƒ íŠ¹ì • GPUì˜ ì—°ì‚°ì´ ëë‚˜ì•¼ ë‹¤ë¥¸ GPUê°€ ì—°ì‚°ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, ì•„ë˜ì˜ ê·¸ë¦¼ì²˜ëŸ¼ Inter-layer ëª¨ë¸ ë³‘ë ¬í™”ëŠ” ë™ì‹œì— í•˜ë‚˜ì˜ GPUë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¹˜ëª…ì ì¸ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](../images/inter_layer_2.png)\n",
    "![](../images/inter_layer_3.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPipe\n",
    "GPipeëŠ” Googleì—ì„œ ê°œë°œëœ íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” ê¸°ë²•ìœ¼ë¡œ Inter Layer ëª¨ë¸ ë³‘ë ¬í™” ì‹œ GPUê°€ ì‰¬ëŠ” ì‹œê°„ (idle time)ì„ ì¤„ì´ê¸° ìœ„í•´ ë“±ì¥í–ˆìœ¼ë©°, mini-batchë¥¼ micro-batchë¡œ í•œë²ˆ ë” ìª¼ê°œì„œ í•™ìŠµ ê³¼ì •ì„ íŒŒì´í”„ë¼ì´ë‹ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/gpipe_1.png)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "![](../images/pipeline_parallelism2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Micro-batch\n",
    "- Mini-batchëŠ” ì „ì²´ ë°ì´í„°ì…‹ì„ nê°œë¡œ ë¶„í• í•œ ì„œë¸Œìƒ˜í”Œ ì§‘í•©ì…ë‹ˆë‹¤.\n",
    "- Micro-batchëŠ” Mini-batchë¥¼ mê°œë¡œ í•œë²ˆ ë” ë¶„í• í•œ ì„œë¸Œìƒ˜í”Œ ì§‘í•©ì…ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/gpipe_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pipelining\n",
    "GPipeëŠ” ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë§ˆì´í¬ë¡œ ë°°ì¹˜ë¡œ ìª¼ê°œê³  ì—°ì‚°ì„ íŒŒì´í”„ë¼ì´ë‹ í•©ë‹ˆë‹¤. ë¶‰ì€ìƒ‰ (GPUê°€ ì‰¬ëŠ” ë¶€ë¶„)ì„ Bubble timeì´ë¼ê³  í•˜ëŠ”ë°, Micro batch ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆ ìˆ˜ë¡ Bubble timeì´ ì¤„ì–´ë“œëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](../images/gpipe_3.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPipe with PyTorch\n",
    "kakaobrainì—ì„œ ê³µê°œí•œ `torchgpipe`ë¥¼ ì‚¬ìš©í•˜ë©´ ì†ì‰½ê²Œ GPipeë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨, `nn.Sequential`ë¡œ ë˜í•‘ëœ ëª¨ë¸ë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë©° ëª¨ë“  ëª¨ë“ˆì˜ ì…ë ¥ê³¼ ì¶œë ¥ íƒ€ì…ì€ `torch.Tensor` í˜¹ì€ `Tuple[torch.Tensor]`ë¡œ ì œí•œë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì½”ë”©í•˜ê¸°ê°€ ìƒë‹¹íˆ ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/gpipe.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgpipe import GPipe\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block as GPT2BlockBase\n",
    "\n",
    "\n",
    "class GPT2Preprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        position_ids = torch.arange(\n",
    "            0, input_shape[-1], dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GPT2Block(GPT2BlockBase):\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = super(GPT2Block, self).forward(\n",
    "            hidden_states=hidden_states,\n",
    "        )\n",
    "        return hidden_states[0]\n",
    "\n",
    "\n",
    "class GPT2Postprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_f = nn.LayerNorm(\n",
    "            config.hidden_size,\n",
    "            eps=config.layer_norm_epsilon,\n",
    "        )\n",
    "        self.lm_head = nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        return lm_logits\n",
    "\n",
    "\n",
    "def create_model_from_pretrained(model_name):\n",
    "    pretrained = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    preprocess = GPT2Preprocessing(pretrained.config)\n",
    "    preprocess.wte.weight = pretrained.transformer.wte.weight\n",
    "    preprocess.wpe.weight = pretrained.transformer.wpe.weight\n",
    "\n",
    "    blocks = pretrained.transformer.h\n",
    "    for i, block in enumerate(blocks):\n",
    "        block.__class__ = GPT2Block\n",
    "\n",
    "    postprocess = GPT2Postprocessing(pretrained.config)\n",
    "    postprocess.ln_f.weight = pretrained.transformer.ln_f.weight\n",
    "    postprocess.ln_f.bias = pretrained.transformer.ln_f.bias\n",
    "    postprocess.lm_head.weight.data = pretrained.lm_head.weight.data.clone()\n",
    "\n",
    "    return nn.Sequential(preprocess, *blocks, postprocess)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 4\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = create_model_from_pretrained(model_name=\"gpt2\")\n",
    "    model = GPipe(\n",
    "        model,\n",
    "        balance=[4, 3, 3, 4],\n",
    "        devices=[0, 1, 2, 3],\n",
    "        chunks=world_size,\n",
    "    )\n",
    "\n",
    "    datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "    datasets = [str(sample) for sample in datasets]\n",
    "    data_loader = DataLoader(datasets, batch_size=8, num_workers=8)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=3e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        tokens = tokenizer(data, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        input_ids = tokens.input_ids.to(0)\n",
    "        labels = tokens.input_ids.to(world_size - 1)\n",
    "\n",
    "        lm_logits = model(input_ids)\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = nn.CrossEntropyLoss()(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"step: {i}, loss: {loss}\")\n",
    "        if i == 300:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 660.36it/s]\n",
      "step: 0, loss: 6.082664489746094\n",
      "step: 10, loss: 3.2473177909851074\n",
      "step: 20, loss: 2.801823616027832\n",
      "step: 30, loss: 2.5457603931427\n",
      "step: 40, loss: 2.847094774246216\n",
      "step: 50, loss: 2.3607358932495117\n",
      "step: 60, loss: 2.537778854370117\n",
      "step: 70, loss: 2.244539976119995\n",
      "step: 80, loss: 2.473845958709717\n",
      "step: 90, loss: 2.94407320022583\n",
      "step: 100, loss: 2.825377941131592\n"
     ]
    }
   ],
   "source": [
    "# !python -m torch.distributed.launch --nproc_per_node=4 ../src/gpipe.py\n",
    "!python ../src/gpipe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. 1F1B Pipelining (PipeDream)\n",
    "\n",
    "Microsoftì—ì„œ ê³µê°œí•œ `PipeDream`ì€ `GPipe`ì™€ëŠ” ì•½ê°„ ë‹¤ë¥¸ ë°©ì‹ì˜ íŒŒì´í”„ë¼ì´ë‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. í”íˆ ì´ ë°©ë²•ì„ 1F1Bë¼ê³  ë¶€ë¥´ëŠ”ë°, ëª¨ë“  Forwardê°€ ëë‚˜ê³  ë‚˜ì„œ Backwardë¥¼ ìˆ˜í–‰í•˜ëŠ” GPipeì™€ ë‹¬ë¦¬ `PipeDream`ì€ Forwardì™€ Backwardë¥¼ ë²ˆê°ˆì•„ê°€ë©´ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"../images/1f1b.png\" width=600>\n",
    "\n",
    "1F1B Pipeliningì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‘ê°€ì§€ ì±Œë¦°ì§€ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "1. Weight version managing\n",
    "2. Work partitioning\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) Weight version managinig\n",
    "GPipeì˜ ê²½ìš° í•˜ë‚˜ì˜ weight ë²„ì „ë§Œ ìš´ìš©í•˜ì§€ë§Œ ì£¼ê¸°ì ìœ¼ë¡œ Pipeline flushê°€ ì¼ì–´ë‚©ë‹ˆë‹¤. Pipeline flushë€ ê³„ì‚°ëœ Gradientë¥¼ í†µí•´ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ flush ê³¼ì • ì¤‘ì—ëŠ” ì–´ë– í•œ forward, backward ì—°ì‚°ë„ í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì²˜ë¦¬ íš¨ìœ¨ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"../images/pipeline_flush.png\" width=600>\n",
    "\n",
    "PipeDreamì€ ì´ëŸ¬í•œ flush ì—†ì´ ê³„ì†í•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•´ë‚˜ê°‘ë‹ˆë‹¤. ë”°ë¼ì„œ forwardì™€ backwardê°€ ëª¨ë‘ ì‰¬ëŠ” ì‹œê°„ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë¥¼ ìœ„í•´ì„œëŠ” ì—¬ëŸ¬ ë²„ì „ì˜ íŒŒë¼ë¯¸í„° ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ ìµœì‹ ë²„ì „ì˜ íŒŒë¼ë¯¸í„°ë§Œ ì €ì¥í•˜ê³  ìˆìœ¼ë©´ ì´ì „ layerì˜ ì¶œë ¥ì´ ë‹¤ìŒ layerë¡œ ì „ì†¡ë  ë•Œ, ë‹¤ìŒ layer ë¶€ë¶„ì´ ì—…ë°ì´íŠ¸ ë  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì´ì£ .\n",
    "\n",
    "<img src=\"../images/1f1b.gif\" width=800>\n",
    "\n",
    "ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë§‰ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²„ì „ì˜ weightë¥¼ ì €ì¥í•˜ì—¬ ê´€ë¦¬í•˜ëŠ”ë° ê·¸ëŸ¬ë©´ weightë¥¼ ì €ì¥í•˜ë©´ ë©”ëª¨ë¦¬ ê³µê°„ì„ ë§ì´ ì°¨ì§€í•˜ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ë¶€ë¶„ì—ì„œ íŠ¸ë ˆì´ë“œ ì˜¤í”„ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
    "- GPipe: ë©”ëª¨ë¦¬ íš¨ìœ¨ì , í”„ë¡œì„¸ì‹± ë¹„íš¨ìœ¨ì \n",
    "- PipeDream: ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì , í”„ë¡œì„¸ì‹± íš¨ìœ¨ì \n",
    "  \n",
    "<br>\n",
    "\n",
    "### 2) Work Partitioning\n",
    "ë‘ë²ˆì¨° ë¬¸ì œëŠ” ë‰´ëŸ´ë„·ì„ ì–´ë–»ê²Œ ìª¼ê°¤ê±´ì§€ì— ëŒ€í•œ ë¬¸ì œì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ Layerë³„ë¡œ ë™ì¼í•œ ìˆ˜ì˜ ë ˆì´ì–´ë¥¼ ê°–ê²Œë” í•˜ëŠ” ê²ƒì´ í•­ìƒ ìµœê³ ì˜ ì†”ë£¨ì…˜ì´ë¼ê³  í•  ìˆ˜ëŠ” ì—†ê² ì£ . ìš°ë¦¬ì—ê²Œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ idle timeì„ ìµœì†Œí™”ì„ ìµœì†Œí™” í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ê° íŒŒí‹°ì…˜ì˜ running timeì´ ë¹„ìŠ·í•´ì•¼ê² ì£ . ê·¸ ì´ì™¸ì—ë„ ì¶”ê°€ë¡œ parameter size, activation memory ë“±ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "<img src=\"../images/pipe_dream.png\" width=600>\n",
    "\n",
    "PipeDreamì€ Profilingê³¼ Optimizingì„ í†µí•´ ìµœì ì˜ Partioning ì „ëµì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variations of 1F1B Pipelining\n",
    "\n",
    "PipeDreamì˜ 1F1B íŒŒì´í”„ë¼ì´ë‹ì„ ê°œì„ í•œ ë‘ê°€ì§€ ë²„ì „ì˜ íŒŒì´í”„ë¼ì¸ ì „ëµì„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) PipeDream 2BW (2-buffered weight update)\n",
    "PipeDream 2BWëŠ” PipeDreamì˜ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” íŒŒì´í”„ë¼ì´ë‹ ì¤‘ì— Gradient Accumulationì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—¬ëŸ¬ê°œì˜ Gradientë“¤ì„ ëª¨ì•„ë‘ë‹¤ê°€ í•œë²ˆì— ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì„± ë¬¸ì œë¥¼ í•´ê²°í–ˆì£ . 2BWëŠ” ì´ì „ê³¼ ë‹¬ë¦¬ ë‹¨ ë‘ê°œì˜ weight versionë§Œ ìœ ì§€í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/pipe_dream_2bw.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) PipeDream Flush\n",
    "PipeDream FlushëŠ” 1F1Bì™€ Pipeline Flushë¥¼ ê²°í•©í•œ íŒŒì´í”„ë¼ì´ë‹ ë°©ë²•ì…ë‹ˆë‹¤. ì´ íŒŒì´í”„ë¼ì´ë‹ ë°©ë²•ì€ Flushê°€ ì¼ì–´ë‚˜ê¸° ë•Œë¬¸ì— GPIpeì™€ ë¹„êµí•˜ì—¬ idle timeì€ ë¹„ìŠ·í•˜ë‚˜, forward-backward ê³¼ì •ì—ì„œ ìœ ì§€í•´ì•¼ í•˜ëŠ” **activation memoryê°€ ì¤„ì–´ë“­ë‹ˆë‹¤.** PipeDream FlushëŠ” Flushê°€ ì¼ì–´ë‚˜ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ë²„ì „ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê´€ë¦¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¨ì¼ ê°€ì¤‘ì¹˜ë§Œ ìœ ì§€í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— PipeDream 2BWë³´ë‹¤ë„ ë” ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤. (ì§€ê¸ˆê¹Œì§€ ì†Œê°œë“œë¦° ê¸°ë²•ë“¤ ì¤‘ ê°€ì¥ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤.)\n",
    "\n",
    "![](../images/pipe_dream_flush.png)\n",
    "\n",
    "![](../images/pipe_dream_flush_2.png)\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ì ê¹... ê·¼ë° Activation Memoryê°€ ë­ì•¼?\n",
    "ëŒ€ë¶€ë¶„ì˜ Layerë“¤ì€ Backwardë¥¼ í˜¸ì¶œí•˜ê¸° ì „ì— Forwardì—ì„œ ë‚˜ì˜¨ ì¶œë ¥ê°’ë“¤ì„ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” `torch.autograd.Function`ì„ ì‚¬ìš©í•´ë³´ì‹  ë¶„ë“¤ì€ ì˜ ì•„ì‹¤í…ë°ìš”. `ctx`ë³€ìˆ˜ì— forward ë ˆì´ì–´ì˜ ì¶œë ¥ê°’ë“¤ì„ ì €ì¥í•´ë‘¡ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ì°¸ê³ : https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReLU(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        # input ê°’ì„ ì €ì¥í•˜ê³  ìˆìŒ.\n",
    "        \n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ëŠ” ë¯¸ë¶„ê°’(Gradient)ì„ ê³„ì‚°í• ë•Œ Forward ê³¼ì •ì—ì„œ ì‚¬ìš©í–ˆë˜ ê°’ë“¤ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì˜ˆì‹œë¥¼ ë´…ì‹œë‹¤.\n",
    "\n",
    "![](../images/max_pooling.png)\n",
    "\n",
    "ìœ„ëŠ” Max Pooling ì—°ì‚°ê³¼ ê·¸ì— ëŒ€í•œ Gradientë¥¼ ê³„ì‚°í•œ ê²ƒì…ë‹ˆë‹¤. Backwardë¥¼ ìˆ˜í–‰í• ë•ŒëŠ” [[0.8, 1.2], [0.9, 0.5]]ì™€ ê°™ì€ (2, 2) í…ì„œê°€ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜µë‹ˆë‹¤. ì´ ê°’ì„ ê°€ì§€ê³  ì˜¤ë¥¸ìª½ì˜ Gradient Matrixë¥¼ ì°¾ì•„ë‚´ì•¼ í•˜ëŠ”ë° ë°˜ë“œì‹œ Forwardì—ì„œ ë°›ì•˜ë˜ (4, 4)ì˜ í…ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ í…ì„œë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ê³  ìˆëŠ” ê²ƒì´ì£ . ì´ë ‡ê²Œ Backwardë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ Forward ë‹¹ì‹œì— ì“°ì˜€ë˜ í…ì„œë“¤ì„ ì €ì¥í•´ë‘ê¸° ìœ„í•´ í•„ìš”í•œ ë©”ëª¨ë¦¬ë¥¼ Activation Memoryë¼ê³  í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ Activation Memoryê°€ ë­”ì§€ ì•Œì•˜ìœ¼ë‹ˆ, PipeDreamì„ ì‹¤ìŠµí•´ë³¼ê¹Œìš”? **PipeDream FlushëŠ” MSì˜ ë¶„ì‚°ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ DeepSpeedì— êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.** (ì°¸ê³ : https://github.com/microsoft/DeepSpeed/issues/1110) ë”°ë¼ì„œ DeepSpeedë¥¼ ì‚¬ìš©í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "### DeepSpeed ëª…ë ¹ì–´ ì‚¬ìš©ë²•\n",
    "ì•„ ì°¸, ê·¸ ì „ì— `deepspeed`ê°€ ì œê³µí•˜ëŠ” ë§¤ìš° í¸ë¦¬í•œ ê¸°ëŠ¥ì„ ë¨¼ì € ì•Œì•„ë³´ê³  ê°€ê² ìŠµë‹ˆë‹¤. ê¸°ì¡´ì—ëŠ” ë¶„ì‚°ì²˜ë¦¬ë¥¼ ìœ„í•´ `python -m torch.distributed.launch --nproc_per_node=n OOO.py`ë¥¼ ì‚¬ìš©í–ˆìœ¼ë‚˜ ë„ˆë¬´ ê¸¸ì–´ì„œ ë¶ˆí¸í–ˆì£ . DeepSpeedëŠ” `deepspeed` í˜¹ì€ `ds`ì™€ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "- `ds --num_gpus=n OOO.py`\n",
    "- `deepspeed --num_gpus=n OOO.py`\n",
    "\n",
    "ìœ„ì™€ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ë©´ `torch.distributed.launch`ì™€ ë™ì¼í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤. ì´ì œë¶€í„°ëŠ” ëª¨ë“  ë¶„ì‚°ì²˜ë¦¬ í”„ë¡œê·¸ë¨ì— `deepspeed`ì˜ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. (ì†”ì§íˆ `torch.distributed.launch`ëŠ” ë„ˆë¬´ ê¸¸ì–´ìš” ğŸ˜­)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/pipe_dream.py\n",
    "\"\"\"\n",
    "import deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from deepspeed import PipelineModule\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block as GPT2BlockBase\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class GPT2Preprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        position_ids = torch.arange(\n",
    "            0, input_shape[-1], dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GPT2Block(GPT2BlockBase):\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = super(GPT2Block, self).forward(\n",
    "            hidden_states=hidden_states,\n",
    "        )\n",
    "        return hidden_states[0]\n",
    "\n",
    "\n",
    "class GPT2Postprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_f = nn.LayerNorm(\n",
    "            config.hidden_size,\n",
    "            eps=config.layer_norm_epsilon,\n",
    "        )\n",
    "        self.lm_head = nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        return lm_logits\n",
    "\n",
    "\n",
    "def create_model_from_pretrained(model_name):\n",
    "    pretrained = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    preprocess = GPT2Preprocessing(pretrained.config)\n",
    "    preprocess.wte.weight = pretrained.transformer.wte.weight\n",
    "    preprocess.wpe.weight = pretrained.transformer.wpe.weight\n",
    "\n",
    "    blocks = pretrained.transformer.h\n",
    "    for i, block in enumerate(blocks):\n",
    "        block.__class__ = GPT2Block\n",
    "\n",
    "    postprocess = GPT2Postprocessing(pretrained.config)\n",
    "    postprocess.ln_f.weight = pretrained.transformer.ln_f.weight\n",
    "    postprocess.ln_f.bias = pretrained.transformer.ln_f.bias\n",
    "    postprocess.lm_head.weight.data = pretrained.lm_head.weight.data.clone()\n",
    "\n",
    "    return nn.Sequential(preprocess, *blocks, postprocess)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_encoding = tokenizer.pad(\n",
    "        {\"input_ids\": batch}, padding=\"max_length\", max_length=1024\n",
    "    )\n",
    "    return batch_encoding.input_ids\n",
    "\n",
    "\n",
    "def batch_fn(data):\n",
    "    input_ids = data\n",
    "    labels = data\n",
    "    return input_ids, labels\n",
    "\n",
    "\n",
    "def loss_fn(logits, labels):\n",
    "    logits = logits[..., :-1, :].contiguous()\n",
    "    labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    return nn.CrossEntropyLoss()(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        labels.view(-1),\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    world_size, rank = dist.get_world_size(), dist.get_rank()\n",
    "    batch_size, train_steps = 16, 300\n",
    "    train_samples = batch_size * train_steps\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = PipelineModule(\n",
    "        create_model_from_pretrained(model_name=\"gpt2\"),\n",
    "        loss_fn=loss_fn,\n",
    "        num_stages=world_size,\n",
    "        partition_method=\"type:GPT2Block\"\n",
    "        # partition_methodë¥¼ í†µí•´ ë³‘ë ¬í™” í•˜ê³  ì‹¶ì€ ë ˆì´ì–´ë¥¼ ê³ ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    )\n",
    "    engine, optimizer, _, _ = deepspeed.initialize(\n",
    "        model=model,\n",
    "        optimizer=Adam(model.parameters(), lr=3e-5),\n",
    "        config={\n",
    "            \"train_batch_size\": batch_size,\n",
    "            \"steps_per_print\": 9999999,\n",
    "            # turn off: https://github.com/microsoft/DeepSpeed/issues/1119\n",
    "        },\n",
    "    )\n",
    "    engine.set_batch_fn(batch_fn)\n",
    "\n",
    "    datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "    datasets = [str(sample) for i, sample in enumerate(datasets) if i < train_samples]\n",
    "    datasets = [\n",
    "        tokenizer(data, return_tensors=\"pt\", max_length=1024).input_ids[0]\n",
    "        for data in tqdm(datasets)\n",
    "    ]\n",
    "    data_loader = iter(\n",
    "        DataLoader(\n",
    "            sorted(datasets, key=len, reverse=True),\n",
    "            # uniform length batching\n",
    "            # https://mccormickml.com/2020/07/29/smart-batching-tutorial/\n",
    "            batch_size=batch_size,\n",
    "            num_workers=8,\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        loss = engine.train_batch(data_loader)\n",
    "\n",
    "        if i % 10 == 0 and rank == 0:\n",
    "            print(f\"step: {i}, loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export NCCL_SHM_DISABLE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-11-01 10:13:24,253] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2021-11-01 10:13:24,408] [INFO] [runner.py:360:main] cmd = /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 ../src/pipe_dream.py\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE libnccl-dev=2.8.4-1+cuda11.2\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION 2.8.4-1\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:73:main] 0 NCCL_VERSION 2.8.4-1\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME libnccl-dev\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE libnccl2=2.8.4-1+cuda11.2\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_NAME libnccl2\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:73:main] 0 NV_LIBNCCL_PACKAGE_VERSION 2.8.4-1\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0, 1]}\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:86:main] nnodes=1, num_local_procs=2, node_rank=0\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:102:main] dist_world_size=2\n",
      "[2021-11-01 10:13:25,380] [INFO] [launch.py:104:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
      "Using topology: {ProcessCoord(pipe=0, data=0): 0, ProcessCoord(pipe=1, data=0): 1}\n",
      "[2021-11-01 10:13:57,078] [INFO] [module.py:365:_partition_layers] Partitioning pipeline stages with method type:GPT2Block\n",
      "stage=0 layers=7\n",
      "     0: GPT2Preprocessing\n",
      "     1: GPT2Block\n",
      "     2: GPT2Block\n",
      "     3: GPT2Block\n",
      "     4: GPT2Block\n",
      "     5: GPT2Block\n",
      "     6: GPT2Block\n",
      "stage=1 layers=7\n",
      "     7: GPT2Block\n",
      "     8: GPT2Block\n",
      "     9: GPT2Block\n",
      "    10: GPT2Block\n",
      "    11: GPT2Block\n",
      "    12: GPT2Block\n",
      "    13: GPT2Postprocessing\n",
      "  loss: loss_fn\n",
      "[2021-11-01 10:14:20,478] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.5.4, git-hash=unknown, git-branch=unknown\n",
      "[2021-11-01 10:14:21,075] [INFO] [engine.py:204:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "[2021-11-01 10:14:21,075] [INFO] [engine.py:848:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2021-11-01 10:14:21,075] [INFO] [engine.py:854:_configure_optimizer] Using client Optimizer as basic optimizer\n",
      "[2021-11-01 10:14:21,078] [INFO] [engine.py:870:_configure_optimizer] DeepSpeed Basic Optimizer = Adam\n",
      "[2021-11-01 10:14:21,078] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2021-11-01 10:14:21,078] [INFO] [engine.py:596:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
      "[2021-11-01 10:14:21,078] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2021-11-01 10:14:21,078] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[(0.9, 0.999)]\n",
      "[2021-11-01 10:14:21,078] [INFO] [config.py:940:print] DeepSpeedEngine configuration:\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   allreduce_always_fp32 ........ False\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   amp_enabled .................. False\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   amp_params ................... False\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   checkpoint_tag_validation_enabled  True\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   checkpoint_tag_validation_fail  False\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   curriculum_enabled ........... False\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   curriculum_params ............ False\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   dataloader_drop_last ......... False\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   disable_allgather ............ False\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   dump_state ................... False\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   dynamic_loss_scale_args ...... None\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   eigenvalue_enabled ........... False\n",
      "[2021-11-01 10:14:21,080] [INFO] [config.py:944:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   eigenvalue_layer_num ......... 0\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   eigenvalue_max_iter .......... 100\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   eigenvalue_stability ......... 1e-06\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   eigenvalue_tol ............... 0.01\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   eigenvalue_verbose ........... False\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   elasticity_enabled ........... False\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   fp16_enabled ................. False\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   fp16_master_weights_and_gradients  False\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   fp16_mixed_quantize .......... False\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   global_rank .................. 0\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   gradient_accumulation_steps .. 1\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   gradient_clipping ............ 0.0\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   gradient_predivide_factor .... 1.0\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   loss_scale ................... 0\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   memory_breakdown ............. False\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   optimizer_legacy_fusion ...... False\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   optimizer_name ............... None\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   optimizer_params ............. None\n",
      "[2021-11-01 10:14:21,081] [INFO] [config.py:944:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   pld_enabled .................. False\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   pld_params ................... False\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   prescale_gradients ........... False\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   quantize_change_rate ......... 0.001\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   quantize_groups .............. 1\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   quantize_offset .............. 1000\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   quantize_period .............. 1000\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   quantize_rounding ............ 0\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   quantize_start_bits .......... 16\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   quantize_target_bits ......... 8\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   quantize_training_enabled .... False\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   quantize_type ................ 0\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   quantize_verbose ............. False\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   scheduler_name ............... None\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   scheduler_params ............. None\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   sparse_attention ............. None\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   sparse_gradients_enabled ..... False\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   steps_per_print .............. 9999999\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   tensorboard_enabled .......... False\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   tensorboard_output_path ...... \n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   train_batch_size ............. 4\n",
      "[2021-11-01 10:14:21,082] [INFO] [config.py:944:print]   train_micro_batch_size_per_gpu  4\n",
      "[2021-11-01 10:14:21,083] [INFO] [config.py:944:print]   use_quantizer_kernel ......... False\n",
      "[2021-11-01 10:14:21,083] [INFO] [config.py:944:print]   wall_clock_breakdown ......... False\n",
      "[2021-11-01 10:14:21,083] [INFO] [config.py:944:print]   world_size ................... 1\n",
      "[2021-11-01 10:14:21,083] [INFO] [config.py:944:print]   zero_allow_untested_optimizer  False\n",
      "[2021-11-01 10:14:21,083] [INFO] [config.py:944:print]   zero_config .................. {\n",
      "    \"stage\": 0, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+08, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": false, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": true, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": null, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_fp16_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2021-11-01 10:14:21,083] [INFO] [config.py:944:print]   zero_enabled ................. False\n",
      "[2021-11-01 10:14:21,083] [INFO] [config.py:944:print]   zero_optimization_stage ...... 0\n",
      "[2021-11-01 10:14:21,083] [INFO] [config.py:946:print]   json = {\n",
      "    \"train_batch_size\": 4, \n",
      "    \"steps_per_print\": 9.999999e+06\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.8269073963165283 seconds\n",
      "Time to load utils op: 0.8281557559967041 seconds\n",
      "[2021-11-01 10:14:23,916] [INFO] [engine.py:77:__init__] CONFIG: micro_batches=1 micro_batch_size=4\n",
      "[2021-11-01 10:14:24,009] [INFO] [engine.py:135:__init__] RANK=0 STAGE=0 LAYERS=7 [0, 7) STAGE_PARAMS=81911040 (81.911M) TOTAL_PARAMS=163037184 (163.037M) UNIQUE_PARAMS=163037184 (163.037M)\n",
      "[2021-11-01 10:14:24,010] [INFO] [engine.py:135:__init__] RANK=1 STAGE=1 LAYERS=7 [7, 14) STAGE_PARAMS=81126144 (81.126M) TOTAL_PARAMS=163037184 (163.037M) UNIQUE_PARAMS=163037184 (163.037M)\n",
      "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.85it/s]\n",
      "  0%|                                                   | 0/400 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "  0%|                                                   | 0/400 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:01<00:00, 379.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:01<00:00, 371.91it/s]\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/pipe/engine.py:1066: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  if inputs.grad is not None:\n",
      "step: 0, loss: 7.792413234710693\n",
      "step: 10, loss: 1.0384193658828735\n",
      "step: 20, loss: 0.8012948036193848\n",
      "step: 30, loss: 0.760172963142395\n",
      "step: 40, loss: 0.6638744473457336\n",
      "step: 50, loss: 0.5307620763778687\n",
      "step: 60, loss: 0.5349492430686951\n",
      "step: 70, loss: 0.5354125499725342\n",
      "step: 80, loss: 0.4314558506011963\n",
      "step: 90, loss: 0.3720009922981262\n"
     ]
    }
   ],
   "source": [
    "!ds --num_gpus=2 ../src/pipe_dream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 5. Interleaved Scheduling\n",
    "ì´ì „ì—ëŠ” í•˜ë‚˜ì˜ ìŠ¤í…Œì´ì§€(ì—°ì†ëœ ë ˆì´ì–´ ì§‘í•©)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•´ì„œ ê²°ê³¼ ê°’ì„ ì¶œë ¥í–ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ 8ê°œì˜ ë ˆì´ì–´ê°€ ìˆê³  2ê°œì˜ ë””ë°”ì´ìŠ¤ê°€ ì£¼ì–´ì¡Œë‹¤ê³  ê°€ì •í•œë‹¤ë©´, ì¼ë°˜ì ìœ¼ë¡œ 1ë²ˆ deviceì— 1-4ë²ˆ ë ˆì´ì–´, 2ë²ˆ deviceì— 5-8ë²ˆ ë ˆì´ì–´ì— í• ë‹¹ë˜ê² ì£ . ê·¸ëŸ¬ë©´ 1ë²ˆ deviceëŠ” 1~4ë²ˆ ë ˆì´ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰í•˜ì—¬ ì¶œë ¥í–ˆìŠµë‹ˆë‹¤. (GPipe, 1F1B ëª¨ë‘ ì´ë ‡ê²Œ ë™ì‘í•¨)\n",
    "  \n",
    "![](../images/interleaved_1.png)\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜ **Interleaved Schedulingì€ Bubble timeì„ ê·¹ë„ë¡œ ì¤„ì´ê¸° ìœ„í•´ í•˜ë‚˜ì˜ ìŠ¤í…Œì´ì§€ë¥¼ ì¤‘ì²©í•´ì„œ ì§„í–‰**í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´  1ë²ˆ deviceê°€ 1-4ë²ˆ ë ˆì´ì–´ì— í• ë‹¹ ë˜ì—ˆë‹¤ë©´, 1-2ë²ˆ ë ˆì´ì–´ì˜ ë™ì‹œì— 3-4ë²ˆ ë ˆì´ì–´ë¥¼ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë˜ë©´ Bubble timeì€ ì¤„ì–´ë“¤ì§€ë§Œ í†µì‹ ë¹„ìš©ì´ ì»¤ì§€ê¸° ë•Œë¬¸ì— ì˜ ì¡°ì ˆí•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. (íŠ¸ë ˆì´ë“œ ì˜¤í”„ ì¡´ì¬)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
