{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Programming\n",
    "\n",
    "Large-scale ëª¨ë¸ì€ í¬ê¸°ê°€ í¬ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ëŒ€ì˜ GPUì— ìª¼ê°œì„œ ëª¨ë¸ì„ ì˜¬ë ¤ì•¼ í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ìª¼ê°œì§„ ê° ëª¨ë¸ì˜ ì¡°ê°ë“¤ë¼ë¦¬ ë„¤íŠ¸ì›Œí¬ë¡œ í†µì‹ ì„ í•˜ë©´ì„œ ê°’ì„ ì£¼ê³  ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì»¤ë‹¤ë€ ë¦¬ì†ŒìŠ¤ë¥¼ ì—¬ëŸ¬ëŒ€ì˜ ì»´í“¨í„° í˜¹ì€ ì—¬ëŸ¬ëŒ€ì˜ ì¥ë¹„ì— ë¶„ì‚°ì‹œì¼œì„œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì„ 'ë¶„ì‚°ì²˜ë¦¬'ë¼ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ ì„¸ì…˜ì—ì„œëŠ” PyTorchë¥¼ ì´ìš©í•œ ë¶„ì‚° í”„ë¡œê·¸ë˜ë°ì˜ ê¸°ì´ˆì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-processing with PyTorch\n",
    "\n",
    "ë¶„ì‚°í”„ë¡œê·¸ë˜ë° íŠœí† ë¦¬ì–¼ì— ì•ì„œ PyTorchë¡œ êµ¬í˜„ëœ Multi-processing ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼ì„ ì§„í–‰í•©ë‹ˆë‹¤. ì“°ë ˆë“œ ë° í”„ë¡œì„¸ìŠ¤ì˜ ê°œë… ë“±ì€ Computer Scienece ì „ê³µìë¼ë©´ ìš´ì˜ì²´ì œ ì‹œê°„ì— ë°°ìš°ëŠ” ê²ƒë“¤ì´ë‹ˆ ìƒëµí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë§Œì•½ ì´ëŸ¬í•œ ê°œë…ì— ëŒ€í•´ ì˜ ëª¨ë¥´ì‹ ë‹¤ë©´, êµ¬ê¸€ì— ê²€ìƒ‰í•˜ì‹œê±°ë‚˜ https://www.backblaze.com/blog/whats-the-diff-programs-processes-and-threads/ ì™€ ê°™ì€ ê¸€ì„ ë¨¼ì € ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.\n",
    "\n",
    "### Multi-process í†µì‹ ì— ì“°ì´ëŠ” ê¸°ë³¸ ìš©ì–´\n",
    "- Node: ì¼ë°˜ì ìœ¼ë¡œ ì»´í“¨í„°ë¼ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë…¸ë“œ 3ëŒ€ë¼ê³  í•˜ë©´ ì»´í“¨í„° 3ëŒ€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "- Global Rank: ì›ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ **MLì—ì„œëŠ” GPUì˜ ID**ë¼ê³  ë³´ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "- Local Rank: ì›ë˜ëŠ” í•œ ë…¸ë“œë‚´ì—ì„œì˜ í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ **MLì—ì„œëŠ” ë…¸ë“œë‚´ì˜ GPU ID**ë¼ê³  ë³´ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "- World Size: í”„ë¡œì„¸ìŠ¤ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/process_terms.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Multi-process Application ì‹¤í–‰ ë°©ë²•\n",
    "PyTorchë¡œ êµ¬í˜„ëœ Multi-process ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰ì‹œí‚¤ëŠ” ë°©ë²•ì€ í¬ê²Œ ë‘ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ íŠ¹ì • í•¨ìˆ˜ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.\n",
    "2. PyTorch ëŸ°ì²˜ê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.\n",
    "\n",
    "ì´ ë‘ê°€ì§€ ë°©ë²•ì— ëŒ€í•´ ëª¨ë‘ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì´ë•Œ, 'ë¶„ê¸°í•œë‹¤.'ë¼ëŠ” í‘œí˜„ì´ ë‚˜ì˜¤ëŠ”ë°, ì´ëŠ” í•œ í”„ë¡œì„¸ìŠ¤ê°€ ë¶€ëª¨ê°€ ë˜ì–´ ì—¬ëŸ¬ê°œì˜ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¥¼ ë™ì‹œì— ì‹¤í–‰ì‹œí‚¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ íŠ¹ì • í•¨ìˆ˜ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.\n",
    "\n",
    "ì´ ë°©ì‹ì€ ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ë©° íŠ¹ì • functionì„ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œì¨ ë¶„ê¸°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/multi_process_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "ì¼ë°˜ì ìœ¼ë¡œ `Spawn`ê³¼ `Fork` ë“± ë‘ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¥¼ ë¶„ê¸° í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- `Spawn`\n",
    "  - ë©”ì¸í”„ë¡œì„¸ìŠ¤ì˜ ìì›ì„ ë¬¼ë ¤ì£¼ì§€ ì•Šê³  í•„ìš”í•œ ë§Œí¼ì˜ ìì›ë§Œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ê²Œ ìƒˆë¡œ í• ë‹¹.\n",
    "  - ì†ë„ê°€ ëŠë¦¬ì§€ë§Œ ì•ˆì „í•œ ë°©ì‹.\n",
    "- `Fork`\n",
    "  - ë©”ì¸í”„ë¡œì„¸ìŠ¤ì˜ ëª¨ë“  ìì›ì„ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì™€ ê³µìœ í•˜ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘.\n",
    "  - ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ ìœ„í—˜í•œ ë°©ì‹.\n",
    "  \n",
    "p.s. ì‹¤ì œë¡œëŠ” `Forkserver` ë°©ì‹ë„ ìˆì§€ë§Œ ìì£¼ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ìƒì†Œí•œ ë°©ì‹ì´ê¸°ì— ìƒëµí•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/multi_process_1.py\n",
    "\n",
    "ì°¸ê³ :\n",
    "Jupyter notebookì€ ë©€í‹°í”„ë¡œì„¸ì‹± ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ë™í•˜ëŠ”ë°ì— ë§ì€ ì œì•½ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "ë”°ë¼ì„œ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì´ê³³ì—ëŠ” ì½”ë“œë§Œ ë™ë´‰í•˜ê³  ì‹¤í–‰ì€ `src` í´ë”ì— ìˆëŠ” ì½”ë“œë¥¼ ë™ì‘ì‹œí‚¤ê² ìŠµë‹ˆë‹¤.\n",
    "ì‹¤ì œ ì½”ë“œ ë™ì‘ì€ `src` í´ë”ì— ìˆëŠ” ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "# ì¼ë°˜ì ìœ¼ë¡œ mpì™€ ê°™ì€ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "# ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ì˜ì—­\n",
    "def fn(rank, param1, param2):\n",
    "    print(f\"{param1} {param2} - rank: {rank}\")\n",
    "\n",
    "\n",
    "# ë©”ì¸ í”„ë¡œì„¸ìŠ¤\n",
    "if __name__ == \"__main__\":\n",
    "    processes = []\n",
    "    # ì‹œì‘ ë°©ë²• ì„¤ì •\n",
    "    mp.set_start_method(\"spawn\")\n",
    "\n",
    "    for rank in range(4):\n",
    "        process = mp.Process(target=fn, args=(rank, \"A0\", \"B1\"))\n",
    "        # ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ìƒì„±\n",
    "        process.daemon = False\n",
    "        # ë°ëª¬ ì—¬ë¶€ (ë©”ì¸í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œì‹œ í•¨ê»˜ ì¢…ë£Œ)\n",
    "        process.start()\n",
    "        # ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
    "        processes.append(process)\n",
    "\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "        # ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ join (=ì™„ë£Œë˜ë©´ ì¢…ë£Œ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0 B1 - rank: 3\n",
      "A0 B1 - rank: 2\n",
      "A0 B1 - rank: 1\n",
      "A0 B1 - rank: 0\n"
     ]
    }
   ],
   "source": [
    "!python ../src/multi_process_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.multiprocessing.spawn` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ ì´ ê³¼ì •ì„ ë§¤ìš° ì‰½ê²Œ ì§„í–‰ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/multi_process_2.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "# ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ì˜ì—­\n",
    "def fn(rank, param1, param2):\n",
    "    # rankëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë“¤ì–´ì˜´. param1, param2ëŠ” spawnì‹œì— ì…ë ¥ë¨.\n",
    "    print(f\"{param1} {param2} - rank: {rank}\")\n",
    "\n",
    "\n",
    "# ë©”ì¸ í”„ë¡œì„¸ìŠ¤\n",
    "if __name__ == \"__main__\":\n",
    "    mp.spawn(\n",
    "        fn=fn,\n",
    "        args=(\"A0\", \"B1\"),\n",
    "        nprocs=4,  # ë§Œë“¤ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜\n",
    "        join=True,  # í”„ë¡œì„¸ìŠ¤ join ì—¬ë¶€\n",
    "        daemon=False,  # ë°ëª¬ ì—¬ë¶€\n",
    "        start_method=\"spawn\",  # ì‹œì‘ ë°©ë²• ì„¤ì •\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0 B1 - rank: 0\n",
      "A0 B1 - rank: 1\n",
      "A0 B1 - rank: 2\n",
      "A0 B1 - rank: 3\n"
     ]
    }
   ],
   "source": [
    "!python ../src/multi_process_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ì°¸ê³ : torch/multiprocessing/spawn.py\n",
    "\n",
    "mp.spawn í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "def start_processes(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn'):\n",
    "    _python_version_check()\n",
    "    mp = multiprocessing.get_context(start_method)\n",
    "    error_queues = []\n",
    "    processes = []\n",
    "    for i in range(nprocs):\n",
    "        error_queue = mp.SimpleQueue()\n",
    "        process = mp.Process(\n",
    "            target=_wrap,\n",
    "            args=(fn, i, args, error_queue),\n",
    "            daemon=daemon,\n",
    "        )\n",
    "        process.start()\n",
    "        error_queues.append(error_queue)\n",
    "        processes.append(process)\n",
    "\n",
    "    context = ProcessContext(processes, error_queues)\n",
    "    if not join:\n",
    "        return context\n",
    "\n",
    "    # Loop on join until it returns True or raises an exception.\n",
    "    while not context.join():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2) PyTorch ëŸ°ì²˜ê°€ ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.\n",
    "\n",
    "ì´ ë°©ì‹ì€ torchì— ë‚´ì¥ëœ ë©€í‹°í”„ë¡œì„¸ì‹± ëŸ°ì²˜ê°€ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ì‹œì¼œì£¼ëŠ” ë§¤ìš° í¸ë¦¬í•œ ë°©ì‹ì…ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/multi_process_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "`torchrun --nproc_per_node=n OOO.py`ì™€ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/multi_process_3.py\n",
    "\"\"\"\n",
    "\n",
    "# ì½”ë“œ ì „ì²´ê°€ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ê°€ ë©ë‹ˆë‹¤.\n",
    "import os\n",
    "\n",
    "# RANK, LOCAL_RANK, WORLD_SIZE ë“±ì˜ ë³€ìˆ˜ê°€ ìë™ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.\n",
    "print(f\"hello world, {os.environ['RANK']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "hello world, 0\n",
      "hello world, 1\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/multi_process_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distributed Programming with PyTorch\n",
    "### Concept of Message Passing\n",
    "\n",
    "ë©”ì‹œì§€ íŒ¨ì‹±ì´ë€ ë™ì¼í•œ ì£¼ì†Œê³µê°„ì„ ê³µìœ í•˜ì§€ ì•ŠëŠ” ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ë“¤ì´ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ì„ ìˆ˜ ìˆë„ë¡ ë©”ì‹œì§€ë¼ëŠ” ê°„ì ‘ ì •ë³´ë¥¼ ì£¼ê³  ë°›ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ Process-1ì´ íŠ¹ì • íƒœê·¸ê°€ ë‹¬ë¦° ë°ì´í„°ë¥¼ ë©”ì‹œì§€ íì— sendí•˜ë„ë¡, Process-2ê°€ í•´ë‹¹ ë°ì´í„°ë¥¼ receiveí•˜ë„ë¡ ì½”ë”©í•´ë†“ìœ¼ë©´ ë‘ í”„ë¡œì„¸ìŠ¤ê°€ ê³µìœ í•˜ëŠ” ë©”ëª¨ë¦¬ ê³µê°„ ì—†ì´ë„ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ì„ ìˆ˜ ìˆì£ . Large-scale ëª¨ë¸ ê°œë°œì‹œì— ì‚¬ìš©ë˜ëŠ” ë¶„ì‚° í†µì‹ ì—ëŠ” ëŒ€ë¶€ë¶„ ì´ëŸ¬í•œ message passing ê¸°ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/message_passing.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### MPI (Massage Passing Interface)\n",
    "MPIëŠ” Message Passingì— ëŒ€í•œ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. MPIì—ëŠ” Processê°„ì˜ Message Passingì— ì‚¬ìš©ë˜ëŠ” ì—¬ëŸ¬ ì—°ì‚°(e.g. broadcast, reduce, scatter, gather, ...)ì´ ì •ì˜ë˜ì–´ ìˆìœ¼ë©° ëŒ€í‘œì ìœ¼ë¡œ OpenMPIë¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/open_mpi.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### NCCL & GLOO\n",
    "ì‹¤ì œë¡œëŠ” openmpi ë³´ë‹¤ëŠ” ncclì´ë‚˜ gloo ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "- NCCL (NVIDIA Collective Communication Library)\n",
    "  - NVIDIAì—ì„œ ê°œë°œí•œ GPU íŠ¹í™” Message Passing ë¼ì´ë¸ŒëŸ¬ë¦¬ ('nickel'ì´ë¼ê³  ì½ìŒ)\n",
    "  - NVIDIA GPUì—ì„œ ì‚¬ìš©ì‹œ, ë‹¤ë¥¸ ë„êµ¬ì— ë¹„í•´ ì›”ë“±íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ìˆìŠµë‹ˆë‹¤.\n",
    "- GLOO (Facebook's Collective Communication Library)\n",
    "  - Facebookì—ì„œ ê°œë°œëœ Message Passing ë¼ì´ë¸ŒëŸ¬ë¦¬. \n",
    "  - `torch`ì—ì„œëŠ” ì£¼ë¡œ CPU ë¶„ì‚°ì²˜ë¦¬ì— ì‚¬ìš©í•˜ë¼ê³  ì¶”ì²œí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "### ë°±ì—”ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ ê°€ì´ë“œ\n",
    "openmpië¥¼ ì¨ì•¼í•  íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼ë©´ ncclì´ë‚˜ glooë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, GPUì—ì„œ ì‚¬ìš©ì‹œ nccl, CPUì—ì„œ ì‚¬ìš©ì‹œ glooë¥¼ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë” ìì„¸í•œ ì •ë³´ëŠ”  https://pytorch.org/docs/stable/distributed.html ì—¬ê¸°ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n",
    "\n",
    "ê° ë°±ì—”ë“œë³„ë¡œ ìˆ˜í–‰ ê°€ëŠ¥í•œ ì—°ì‚°ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](../images/backends.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### `torch.distributed` íŒ¨í‚¤ì§€\n",
    "`gloo`, `nccl`, `openmpi` ë“±ì„ ì§ì ‘ ì‚¬ìš©í•´ë³´ëŠ” ê²ƒì€ ë¶„ëª… ì¢‹ì€ ê²½í—˜ì´ ë  ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹œê°„ ê´€ê³„ìƒ ì´ë“¤ì„ ëª¨ë‘ ë‹¤ë£° ìˆ˜ëŠ” ì—†ê³ , ì´ë“¤ì„ wrapping í•˜ê³  ìˆëŠ” `torch.distributed` íŒ¨í‚¤ì§€ë¥¼  ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ í™œìš© ë‹¨ìœ¼ë¡œ ê°€ë©´ `nccl` ë“±ì„ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëŒ€ë¶€ë¶„ì˜ ê²½ìš° `torch.distributed` ë“±ì˜ í•˜ì´ë ˆë²¨ íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œê·¸ë˜ë° í•˜ê²Œ ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Group\n",
    "\n",
    "ë§ì€ í”„ë¡œì„¸ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ì¼ì…ë‹ˆë‹¤. ë”°ë¼ì„œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ë§Œë“¤ì–´ì„œ ê´€ë¦¬ë¥¼ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. `init_process_group`ë¥¼ í˜¸ì¶œí•˜ë©´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ê°€ ì†í•œ default_pg(process group)ê°€ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ì´ˆê¸°í™”í•˜ëŠ” `init_process_group` í•¨ìˆ˜ëŠ” **ë°˜ë“œì‹œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰**ë˜ì–´ì•¼ í•˜ë©°, ë§Œì•½ ì¶”ê°€ë¡œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë“¤ë§Œ ëª¨ì•„ì„œ ê·¸ë£¹ì„ ìƒì„±í•˜ë ¤ë©´ `new_group`ì„ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/process_group_1.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.distributed as dist\n",
    "# ì¼ë°˜ì ìœ¼ë¡œ distì™€ ê°™ì€ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "dist.init_process_group(backend=\"nccl\", rank=0, world_size=1)\n",
    "# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”\n",
    "# ë³¸ ì˜ˆì œì—ì„œëŠ” ê°€ì¥ ìì£¼ ì‚¬ìš©í•˜ëŠ” ncclì„ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "# backendì— 'nccl' ëŒ€ì‹  'mpi'ë‚˜ 'gloo'ë¥¼ ë„£ì–´ë„ ë©ë‹ˆë‹¤.\n",
    "\n",
    "process_group = dist.new_group([0])\n",
    "# 0ë²ˆ í”„ë¡œì„¸ìŠ¤ê°€ ì†í•œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„±\n",
    "\n",
    "print(process_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"../src/process_group_1.py\", line 8, in <module>\r\n",
      "    dist.init_process_group(backend=\"nccl\", rank=0, world_size=1)\r\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 576, in init_process_group\r\n",
      "    store, rank, world_size = next(rendezvous_iterator)\r\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 226, in _env_rendezvous_handler\r\n",
      "    master_addr = _get_env_or_raise(\"MASTER_ADDR\")\r\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 203, in _get_env_or_raise\r\n",
      "    raise _env_error(env_var)\r\n",
      "ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set\r\n"
     ]
    }
   ],
   "source": [
    "!python ../src/process_group_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” `MASTER_ADDR`, `MASTER_PORT` ë“± í•„ìš”í•œ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ ê°’ë“¤ì„ ì„¤ì •í•˜ê³  ë‹¤ì‹œ ì‹¤í–‰ì‹œí‚¤ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/process_group_2.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "\n",
    "# ì¼ë°˜ì ìœ¼ë¡œ ì´ ê°’ë“¤ë„ í™˜ê²½ë³€ìˆ˜ë¡œ ë“±ë¡í•˜ê³  ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "# í†µì‹ ì— í•„ìš”í•œ ì£¼ì†Œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"  # í†µì‹ í•  ì£¼ì†Œ (ë³´í†µ localhostë¥¼ ì”ë‹ˆë‹¤.)\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"  # í†µì‹ í•  í¬íŠ¸ (ì„ì˜ì˜ ê°’ì„ ì„¤ì •í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.)\n",
    "\n",
    "dist.init_process_group(backend=\"nccl\", rank=0, world_size=1)\n",
    "# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”\n",
    "\n",
    "process_group = dist.new_group([0])\n",
    "# 0ë²ˆ í”„ë¡œì„¸ìŠ¤ê°€ ì†í•œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„±\n",
    "\n",
    "print(process_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f571bff92b0>\r\n"
     ]
    }
   ],
   "source": [
    "!python ../src/process_group_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ ì˜ˆì œëŠ” í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì˜ APIë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ì„œ ë©”ì¸í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. (í”„ë¡œì„¸ìŠ¤ê°€ 1ê°œ ë¿ì´ë¼ ìƒê´€ì—†ì—ˆìŒ) ì‹¤ì œë¡œëŠ” ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‘ì—…ì‹œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„± ë“±ì˜ ì‘ì—…ì€ ë°˜ë“œì‹œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/process_group_3.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "\n",
    "# ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ì˜ì—­\n",
    "def fn(rank, world_size):\n",
    "    # rankëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë“¤ì–´ì˜´. world_sizeëŠ” ì…ë ¥ë¨.\n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    group = dist.new_group([_ for _ in range(world_size)])\n",
    "    print(f\"{group} - rank: {rank}\")\n",
    "\n",
    "\n",
    "# ë©”ì¸ í”„ë¡œì„¸ìŠ¤\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    os.environ[\"WORLD_SIZE\"] = \"4\"\n",
    "\n",
    "    mp.spawn(\n",
    "        fn=fn,\n",
    "        args=(4,),  # world_size ì…ë ¥\n",
    "        nprocs=4,  # ë§Œë“¤ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜\n",
    "        join=True,  # í”„ë¡œì„¸ìŠ¤ join ì—¬ë¶€\n",
    "        daemon=False,  # ë°ëª¬ ì—¬ë¶€\n",
    "        start_method=\"spawn\",  # ì‹œì‘ ë°©ë²• ì„¤ì •\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7ff9691388f0> - rank: 0\r\n",
      "<torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fa0ffa4afb0> - rank: 2\r\n",
      "<torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fe610cac5f0> - rank: 1\r\n",
      "<torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f950ecadf30> - rank: 3\r\n"
     ]
    }
   ],
   "source": [
    "!python ../src/process_group_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchrun --nproc_per_node=n OOO.py`ë¥¼ ì‚¬ìš©í• ë•ŒëŠ” ì•„ë˜ì™€ ê°™ì´ ì²˜ë¦¬í•©ë‹ˆë‹¤. `dist.get_rank()`, `dist_get_world_size()`ì™€ ê°™ì€ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ `rank`ì™€ `world_size`ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/process_group_4.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(backend=\"nccl\")\n",
    "# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”\n",
    "\n",
    "group = dist.new_group([_ for _ in range(dist.get_world_size())])\n",
    "# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„±\n",
    "\n",
    "print(f\"{group} - rank: {dist.get_rank()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "<torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7fee7b5032b0> - rank: 0\n",
      "\n",
      "<torch._C._distributed_c10d.ProcessGroupNCCL object at 0x7f0a7f165c30> - rank: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/process_group_4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2P Communication (Point to point) \n",
    "\n",
    "![](../images/p2p.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "P2P (Point to point, ì  ëŒ€ ì ) í†µì‹ ì€ íŠ¹ì • í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ëŠ” í†µì‹ ì´ë©° `torch.distributed` íŒ¨í‚¤ì§€ì˜ `send`, `recv` í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í†µì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/p2p_communication.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"gloo\")\n",
    "# í˜„ì¬ ncclì€ send, recvë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (2021/10/21)\n",
    "\n",
    "if dist.get_rank() == 0:\n",
    "    tensor = torch.randn(2, 2)\n",
    "    dist.send(tensor, dst=1)\n",
    "\n",
    "elif dist.get_rank() == 1:\n",
    "    tensor = torch.zeros(2, 2)\n",
    "    print(f\"rank 1 before: {tensor}\\n\")\n",
    "    dist.recv(tensor, src=0)\n",
    "    print(f\"rank 1 after: {tensor}\\n\")\n",
    "\n",
    "else:\n",
    "    raise RuntimeError(\"wrong rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "rank 1 before: tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "rank 1 after: tensor([[-1.1546, -1.5837],\n",
      "        [ 0.3939, -0.2360]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/p2p_communication.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì£¼ì˜í•  ê²ƒì€ ì´ë“¤ì´ ë™ê¸°ì ìœ¼ë¡œ í†µì‹ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ë¹„ë™ê¸° í†µì‹ (non-blocking)ì—ëŠ” `isend`, `irecv`ë¥¼ ì´ìš©í•©ë‹ˆë‹¤. ì´ë“¤ì€ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— `wait()` ë©”ì„œë“œë¥¼ í†µí•´ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì˜ í†µì‹ ì´ ëë‚ ë•Œ ê¹Œì§€ ê¸°ë‹¤ë¦¬ê³  ë‚œ ë’¤ì— ì ‘ê·¼í•´ì•¼í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/p2p_communication_non_blocking.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"gloo\")\n",
    "# í˜„ì¬ ncclì€ send, recvë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (2021/10/21)\n",
    "\n",
    "if dist.get_rank() == 0:\n",
    "    tensor = torch.randn(2, 2)\n",
    "    request = dist.isend(tensor, dst=1)\n",
    "elif dist.get_rank() == 1:\n",
    "    tensor = torch.zeros(2, 2)\n",
    "    request = dist.irecv(tensor, src=0)\n",
    "else:\n",
    "    raise RuntimeError(\"wrong rank\")\n",
    "\n",
    "request.wait()\n",
    "\n",
    "print(f\"rank {dist.get_rank()}: {tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "rank 0: tensor([[-2.1953,  2.1798],\n",
      "        [-0.7241, -0.8670]])rank 1: tensor([[-2.1953,  2.1798],\n",
      "        [-0.7241, -0.8670]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/p2p_communication_non_blocking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Collective Communication\n",
    "\n",
    "Collective Communicationì€ ì§‘í•©í†µì‹ ì´ë¼ëŠ” ëœ»ìœ¼ë¡œ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ê°€ ì°¸ì—¬í•˜ì—¬ í†µì‹ í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì—°ì‚°ë“¤ì´ ìˆì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ 4ê°œì˜ ì—°ì‚°(`broadcast`, `scatter`, `gather`, `reduce`)ì´ ê¸°ë³¸ ì„¸íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/collective.png)\n",
    "\n",
    "ì—¬ê¸°ì— ì¶”ê°€ë¡œ `all-reduce`, `all-gather`, `reduce-scatter` ë“±ì˜ ë³µí•© ì—°ì‚°ê³¼ ë™ê¸°í™” ì—°ì‚°ì¸ `barrier`ê¹Œì§€ ì´ 8ê°œ ì—°ì‚°ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ ë§Œì•½ ì´ëŸ¬í•œ ì—°ì‚°ë“¤ì„ ë¹„ë™ê¸° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ë ¤ë©´ ê° ì—°ì‚° ìˆ˜í–‰ì‹œ `async_op` íŒŒë¼ë¯¸í„°ë¥¼ `True`ë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1) Broadcast\n",
    "\n",
    "BroadcastëŠ” íŠ¹ì • í”„ë¡œì„¸ìŠ¤ì— ìˆëŠ” ë°ì´í„°ë¥¼ ê·¸ë£¹ë‚´ì˜ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì— ë³µì‚¬í•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/broadcast.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/broadcast.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "# deviceë¥¼ settingí•˜ë©´ ì´í›„ì— rankì— ë§ëŠ” ë””ë°”ì´ìŠ¤ì— ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "if rank == 0:\n",
    "    tensor = torch.randn(2, 2).to(torch.cuda.current_device())\n",
    "else:\n",
    "    tensor = torch.zeros(2, 2).to(torch.cuda.current_device())\n",
    "\n",
    "print(f\"before rank {rank}: {tensor}\\n\")\n",
    "dist.broadcast(tensor, src=0)\n",
    "print(f\"after rank {rank}: {tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "before rank 1: tensor([[0., 0.],\n",
      "        [0., 0.]], device='cuda:1')\n",
      "\n",
      "before rank 0: tensor([[ 0.3019, -0.3482],\n",
      "        [ 0.7522,  0.7167]], device='cuda:0')\n",
      "\n",
      "after rank 0: tensor([[ 0.3019, -0.3482],\n",
      "        [ 0.7522,  0.7167]], device='cuda:0')\n",
      "\n",
      "after rank 1: tensor([[ 0.3019, -0.3482],\n",
      "        [ 0.7522,  0.7167]], device='cuda:1')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/broadcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`send`, `recv` ë“±ì˜ P2P ì—°ì‚°ì´ ì§€ì›ë˜ì§€ ì•Šì„ë•Œ ì•Šì•„ì„œ `broadcast`ë¥¼ P2P í†µì‹  ìš©ë„ë¡œ ì‚¬ìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤. src=0, dst=1 ì¼ë•Œ, `new_group([0, 1])` ê·¸ë£¹ì„ ë§Œë“¤ê³  `broadcast`ë¥¼ ìˆ˜í–‰í•˜ë©´ 0 -> 1 P2Pì™€ ë™ì¼í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ì°¸ê³ : deepspeed/deepspeed/runtime/pipe/p2p.py\n",
    "\"\"\"\n",
    "\n",
    "def send(tensor, dest_stage, async_op=False):\n",
    "    global _groups\n",
    "    assert async_op == False, \"Doesnt support async_op true\"\n",
    "    src_stage = _grid.get_stage_id()\n",
    "    _is_valid_send_recv(src_stage, dest_stage)\n",
    "\n",
    "    dest_rank = _grid.stage_to_global(stage_id=dest_stage)\n",
    "    if async_op:\n",
    "        global _async\n",
    "        op = dist.isend(tensor, dest_rank)\n",
    "        _async.append(op)\n",
    "    else:\n",
    "\n",
    "        if can_send_recv():\n",
    "            return dist.send(tensor, dest_rank)\n",
    "        else:\n",
    "            group = _get_send_recv_group(src_stage, dest_stage)\n",
    "            src_rank = _grid.stage_to_global(stage_id=src_stage)\n",
    "            return dist.broadcast(tensor, src_rank, group=group, async_op=async_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 2) Reduce\n",
    "ReduceëŠ” ê° í”„ë¡œì„¸ìŠ¤ê°€ ê°€ì§„ ë°ì´í„°ë¡œ íŠ¹ì • ì—°ì‚°ì„ ìˆ˜í–‰í•´ì„œ ì¶œë ¥ì„ í•˜ë‚˜ì˜ ë””ë°”ì´ìŠ¤ë¡œ ëª¨ì•„ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤. ì—°ì‚°ì€ ì£¼ë¡œ sum, max, min ë“±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/reduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/reduce_sum.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [[0, 0], [0, 0]]\n",
    "# rank==1 => [[1, 1], [1, 1]]\n",
    "# rank==2 => [[2, 2], [2, 2]]\n",
    "# rank==3 => [[3, 3], [3, 3]]\n",
    "\n",
    "dist.reduce(tensor, op=torch.distributed.ReduceOp.SUM, dst=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/reduce_sum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/reduce_max.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [[0, 0], [0, 0]]\n",
    "# rank==1 => [[1, 1], [1, 1]]\n",
    "# rank==2 => [[2, 2], [2, 2]]\n",
    "# rank==3 => [[3, 3], [3, 3]]\n",
    "\n",
    "dist.reduce(tensor, op=torch.distributed.ReduceOp.MAX, dst=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/reduce_max.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Scatter\n",
    "ScatterëŠ” ì—¬ëŸ¬ê°œì˜ elementë¥¼ ìª¼ê°œì„œ ê° deviceì— ë¿Œë ¤ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/scatter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/scatter.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"gloo\")\n",
    "# ncclì€ scatterë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "\n",
    "output = torch.zeros(1)\n",
    "print(f\"before rank {rank}: {output}\\n\")\n",
    "\n",
    "if rank == 0:\n",
    "    inputs = torch.tensor([10.0, 20.0])\n",
    "    inputs = torch.split(inputs, dim=0, split_size_or_sections=1)\n",
    "    # (tensor([10]), tensor([20]), tensor([30]), tensor([40]))\n",
    "    dist.scatter(output, scatter_list=list(inputs), src=0)\n",
    "else:\n",
    "    dist.scatter(output, src=0)\n",
    "\n",
    "print(f\"after rank {rank}: {output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "before rank 0: tensor([0.])\n",
      "\n",
      "before rank 1: tensor([0.])\n",
      "\n",
      "after rank 0: tensor([10.])\n",
      "after rank 1: tensor([20.])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ncclì—ì„œëŠ” scatterê°€ ì§€ì›ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ scatter ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/scatter_nccl.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "inputs = torch.tensor([10.0, 20.0])\n",
    "inputs = torch.split(tensor=inputs, dim=-1, split_size_or_sections=1)\n",
    "output = inputs[rank].contiguous().to(torch.cuda.current_device())\n",
    "print(f\"after rank {rank}: {output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "after rank 1: tensor([20.], device='cuda:1')\n",
      "\n",
      "after rank 0: tensor([10.], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/scatter_nccl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ì°¸ê³ : megatron-lm/megatron/mpu/mappings.py\n",
    "\"\"\"\n",
    "\n",
    "def _split(input_):\n",
    "    \"\"\"Split the tensor along its last dimension and keep the\n",
    "    corresponding slice.\"\"\"\n",
    "\n",
    "    world_size = get_tensor_model_parallel_world_size()\n",
    "    # Bypass the function if we are using only 1 GPU.\n",
    "    if world_size==1:\n",
    "        return input_\n",
    "\n",
    "    # Split along last dimension.\n",
    "    input_list = split_tensor_along_last_dim(input_, world_size)\n",
    "\n",
    "    # Note: torch.split does not create contiguous tensors by default.\n",
    "    rank = get_tensor_model_parallel_rank()\n",
    "    output = input_list[rank].contiguous()\n",
    "\n",
    "    return output\n",
    "\n",
    "class _ScatterToModelParallelRegion(torch.autograd.Function):\n",
    "    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(graph, input_):\n",
    "        return _split(input_)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_):\n",
    "        return _split(input_)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return _gather(grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 4) Gather\n",
    "GatherëŠ” ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” í…ì„œë¥¼ í•˜ë‚˜ë¡œ ëª¨ì•„ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.\n",
    "![](../images/gather.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/gather.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"gloo\")\n",
    "# ncclì€ gatherë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "input = torch.ones(1) * rank\n",
    "# rank==0 => [0]\n",
    "# rank==1 => [1]\n",
    "# rank==2 => [2]\n",
    "# rank==3 => [3]\n",
    "\n",
    "if rank == 0:\n",
    "    outputs_list = [torch.zeros(1), torch.zeros(1), torch.zeros(1), torch.zeros(1)]\n",
    "    dist.gather(input, gather_list=outputs_list, dst=0)\n",
    "    print(outputs_list)\n",
    "else:\n",
    "    dist.gather(input, dst=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[tensor([0.]), tensor([1.])]\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 5) All-reduce\n",
    "ì´ë¦„ ì•ì— All- ì´ ë¶™ì€ ì—°ì‚°ë“¤ì€ í•´ë‹¹ ì—°ì‚°ì„ ìˆ˜í–‰ í•œë’¤, ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ broadcastí•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ All-reduceëŠ” reduceë¥¼ ìˆ˜í–‰í•œ ë’¤, ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.\n",
    "![](../images/allreduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/allreduce_sum.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [[0, 0], [0, 0]]\n",
    "# rank==1 => [[1, 1], [1, 1]]\n",
    "# rank==2 => [[2, 2], [2, 2]]\n",
    "# rank==3 => [[3, 3], [3, 3]]\n",
    "\n",
    "dist.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)\n",
    "\n",
    "print(f\"rank {rank}: {tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "rank 0: tensor([[1., 1.],\n",
      "        [1., 1.]], device='cuda:0')\n",
      "\n",
      "rank 1: tensor([[1., 1.],\n",
      "        [1., 1.]], device='cuda:1')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/allreduce_sum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/allreduce_max.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [[0, 0], [0, 0]]\n",
    "# rank==1 => [[1, 1], [1, 1]]\n",
    "# rank==2 => [[2, 2], [2, 2]]\n",
    "# rank==3 => [[3, 3], [3, 3]]\n",
    "\n",
    "dist.all_reduce(tensor, op=torch.distributed.ReduceOp.MAX)\n",
    "\n",
    "print(f\"rank {rank}: {tensor}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "rank 0: tensor([[1., 1.],\n",
      "        [1., 1.]], device='cuda:0')\n",
      "\n",
      "rank 1: tensor([[1., 1.],\n",
      "        [1., 1.]], device='cuda:1')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/allreduce_max.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) All-gather\n",
    "All-gatherëŠ” gatherë¥¼ ìˆ˜í–‰í•œ ë’¤, ëª¨ì•„ì§„ ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/allgather.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/allgather.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "input = torch.ones(1).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [0]\n",
    "# rank==1 => [1]\n",
    "# rank==2 => [2]\n",
    "# rank==3 => [3]\n",
    "\n",
    "outputs_list = [\n",
    "    torch.zeros(1, device=torch.device(torch.cuda.current_device())),\n",
    "    torch.zeros(1, device=torch.device(torch.cuda.current_device()))\n",
    "]\n",
    "\n",
    "dist.all_gather(tensor_list=outputs_list, tensor=input)\n",
    "print(outputs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[tensor([0.], device='cuda:0'), tensor([1.], device='cuda:0')]\n",
      "[tensor([0.], device='cuda:1'), tensor([1.], device='cuda:1')]\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/allgather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Reduce-scatter\n",
    "Reduce scatterëŠ” Reduceë¥¼ ìˆ˜í–‰í•œ ë’¤, ê²°ê³¼ë¥¼ ìª¼ê°œì„œ ë””ë°”ì´ìŠ¤ì— ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/reduce_scatter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/reduce_scatter.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "input_list = torch.tensor([1, 10, 100, 1000]).to(torch.cuda.current_device()) * rank\n",
    "input_list = torch.split(input_list, dim=0, split_size_or_sections=1)\n",
    "# rank==0 => [0, 00, 000, 0000]\n",
    "# rank==1 => [1, 10, 100, 1000]\n",
    "# rank==2 => [2, 20, 200, 2000]\n",
    "# rank==3 => [3, 30, 300, 3000]\n",
    "\n",
    "output = torch.tensor([0], device=torch.device(torch.cuda.current_device()),)\n",
    "\n",
    "dist.reduce_scatter(\n",
    "    output=output,\n",
    "    input_list=list(input_list),\n",
    "    op=torch.distributed.ReduceOp.SUM,\n",
    ")\n",
    "\n",
    "print(f\"rank {rank}: {output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "rank 0: tensor([1], device='cuda:0')\n",
      "\n",
      "rank 1: tensor([10], device='cuda:1')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/reduce_scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Barrier\n",
    "BarrierëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ë™ê¸°í™” í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ë¨¼ì € barrierì— ë„ì°©í•œ í”„ë¡œì„¸ìŠ¤ëŠ” ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ í•´ë‹¹ ì§€ì ê¹Œì§€ ì‹¤í–‰ë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/barrier.py\n",
    "\"\"\"\n",
    "import time\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    seconds = 0\n",
    "    while seconds <= 3:\n",
    "        time.sleep(1)\n",
    "        seconds += 1\n",
    "        print(f\"rank 0 - seconds: {seconds}\\n\")\n",
    "\n",
    "print(f\"rank {rank}: no-barrier\\n\")\n",
    "dist.barrier()\n",
    "print(f\"rank {rank}: barrier\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "rank 1: no-barrier\n",
      "\n",
      "rank 0 - seconds: 1\n",
      "\n",
      "rank 0 - seconds: 2\n",
      "\n",
      "rank 0 - seconds: 3\n",
      "\n",
      "rank 0 - seconds: 4\n",
      "\n",
      "rank 0: no-barrier\n",
      "\n",
      "rank 0: barrier\n",
      "\n",
      "rank 1: barrier\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ../src/barrier.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë„ˆë¬´ ë§ì£ ...? ğŸ˜…\n",
    "ì•„ë˜ 4ê°œì˜ ê¸°ë³¸ ì—°ì‚°ë§Œ ì˜ ê¸°ì–µí•´ë‘¬ë„ ëŒ€ë¶€ë¶„ ìœ ì¶”í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](../images/collective.png)\n",
    "\n",
    "4ê°€ì§€ ì—°ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ì˜ ì‚¬í•­ë“¤ì„ ìµí˜€ë‘ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "- `all-reduce`, `all-gather`ëŠ” í•´ë‹¹ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³  ë‚˜ì„œ `broadcast` ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "- `reduce-scatter`ëŠ” ë§ ê·¸ëŒ€ë¡œ `reduce` ì—°ì‚°ì˜ ê²°ê³¼ë¥¼ `scatter (ìª¼ê°œê¸°)` ì²˜ë¦¬í•œë‹¤ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "- `barrier`ëŠ” ì˜ì–´ ëœ» ê·¸ëŒ€ë¡œ ë²½ê³¼ ê°™ì€ ê²ƒì…ë‹ˆë‹¤. ë¨¼ì € ë„ì°©í•œ í”„ë¡œì„¸ìŠ¤ë“¤ì´ ëª» ì§€ë‚˜ê°€ê²Œ ë²½ì²˜ëŸ¼ ë§‰ì•„ë‘ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
